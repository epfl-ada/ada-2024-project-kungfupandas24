{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<h1>Reel Realities: How Gender and Age Shape Success Across Box Office and Streaming Platforms</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <u>Data cleaning and pre-processing</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 CMU Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use three files of the CMU dataset:\n",
    "1. \"plot_summaries.txt\" gives us the plots of the movies.\n",
    "2. \"movie.metadata.tsv\" gives us information about the languages, countries, and genres of the movies.\n",
    "3. \"character.metadata.tsv\" gives us information about actors and the characters they play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_plot_summaries = os.path.join(\"data\",\"CMU\",\"plot_summaries.txt\")\n",
    "plot_summaries_df = pd.read_csv(path_plot_summaries, delimiter=\"\\t\", names = [\"Wikipedia_movie_ID\", \"Plot Summaries\"])\n",
    "\n",
    "print(f\"The plot summaries dataframe has {len(plot_summaries_df):,} values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_id = plot_summaries_df[\"Wikipedia_movie_ID\"].isnull().sum()\n",
    "empty_id = plot_summaries_df[plot_summaries_df[\"Wikipedia_movie_ID\"] == \"\"][\"Wikipedia_movie_ID\"].sum()\n",
    "null_plot = plot_summaries_df[\"Plot Summaries\"].isnull().sum()\n",
    "empty_plot = plot_summaries_df[plot_summaries_df[\"Plot Summaries\"] == \"\"][\"Plot Summaries\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of null values in 'Wikipedia_movie_ID': {null_id}\")\n",
    "print(f\"Number of empty values in 'Wikipedia_movie_ID': {empty_id}\")\n",
    "print(f\"Number of null values in 'Plot Summaries': {null_plot}\")\n",
    "print(f\"Number of empty values in 'Plot Summaries': {empty_plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"Wikipedia_movie_ID\",\n",
    "                \"Freebase_movie_ID\",\n",
    "                \"Movie_name\",\n",
    "                \"Movie_release_date\",\n",
    "                \"Movie_box_office_revenue\",\n",
    "                \"Movie_runtime\",\n",
    "                \"Movie_languages\",\n",
    "                \"Movie_countries\",\n",
    "                \"Movie_genres\"]\n",
    "\n",
    "movie_metadata_df = pd.read_csv(\"data/CMU/movie.metadata.tsv\", delimiter='\\t', names = column_names)\n",
    "print(f\"The movie metadata dataframe has {len(movie_metadata_df):,} values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_metadata_df[\"Movie_name\"] = (movie_metadata_df[\"Movie_name\"]\n",
    "                    .str.strip()  #Remove leading/trailing whitespace\n",
    "                    .str.lower()  #Convert to lowercase\n",
    "                    .replace(\"\", np.nan)  #Replace empty strings with NaN\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie languages, countries, and genres are formatted as a dictionary, but they are actually a string.\n",
    "# We convert them to comma spereated strings. \n",
    "movie_metadata_df[\"Movie_languages\"] = movie_metadata_df[\"Movie_languages\"].apply(ast.literal_eval)\n",
    "movie_metadata_df[\"Movie_languages\"] = movie_metadata_df[\"Movie_languages\"].apply(lambda x: ', '.join(x.values()))\n",
    "\n",
    "movie_metadata_df[\"Movie_countries\"] = movie_metadata_df[\"Movie_countries\"].apply(ast.literal_eval)\n",
    "movie_metadata_df[\"Movie_countries\"] = movie_metadata_df[\"Movie_countries\"].apply(lambda x: ', '.join(x.values()))\n",
    "\n",
    "movie_metadata_df[\"Movie_genres\"] = movie_metadata_df[\"Movie_genres\"].apply(ast.literal_eval)\n",
    "movie_metadata_df[\"Movie_genres\"] = movie_metadata_df[\"Movie_genres\"].apply(lambda x: ', '.join(x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "def clean_string_list(lst):\n",
    "    # Check if lst is a list\n",
    "    if isinstance(lst, list):\n",
    "        return [s.strip().lower() if isinstance(s, str) and s.strip() != \"\" else np.nan for s in lst]\n",
    "    elif isinstance(lst, str): \n",
    "        return lst.strip().lower()\n",
    "    else:\n",
    "        return np.nan \n",
    "\n",
    "movie_metadata_df[\"Movie_languages\"] = movie_metadata_df[\"Movie_languages\"].apply(clean_string_list)\n",
    "movie_metadata_df[\"Movie_countries\"] = movie_metadata_df[\"Movie_countries\"].apply(clean_string_list)\n",
    "movie_metadata_df[\"Movie_genres\"] = movie_metadata_df[\"Movie_genres\"].apply(clean_string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are inconsistencies in the date format: some rows have the date as year only, others as year-month, and some as year-month-day. We convert them all to the year format only since the month and day are not relevant to our analysis.\n",
    "full_date_pattern = r'^\\d{4}-\\d{2}-\\d{2}$' #Matches YYYY-MM-DD\n",
    "year_month_pattern = r'^\\d{4}-\\d{2}$' #Matches YYYY-MM\n",
    "year_only_pattern = r'^\\d{4}$' #Matches YYYY\n",
    "\n",
    "def identify_pattern(date):\n",
    "    if pd.isna(date):\n",
    "        return \"Missing\"\n",
    "    elif re.match(full_date_pattern, date):\n",
    "        return \"Full Date (YYYY-MM-DD)\"\n",
    "    elif re.match(year_month_pattern, date):\n",
    "        return \"Year & Month Date (YYYY-MM)\"\n",
    "    elif re.match(year_only_pattern, date):\n",
    "        return \"Year Only (YYYY)\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "movie_metadata_df[\"Pattern\"] = movie_metadata_df[\"Movie_release_date\"].apply(identify_pattern)\n",
    "pattern_summary = movie_metadata_df.groupby(\"Pattern\").size().reset_index(name=\"Count\")\n",
    "\n",
    "print(\"Pattern Summary:\")\n",
    "print(pattern_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_metadata_df[\"Movie_release_date\"] = movie_metadata_df[\"Movie_release_date\"].apply(lambda x: str(x)[:4] if pd.notnull(x) else None)\n",
    "movie_metadata_df = movie_metadata_df.drop(columns=[\"Pattern\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_null_movie_metadata = ((movie_metadata_df.isnull().sum() / len(movie_metadata_df)) * 100).apply(lambda x: f\"{x:,.2f}%\")\n",
    "\n",
    "print(\n",
    "    f\"Percentage of null values per column:\\n\"\n",
    "    f\"movie_metadata_df:\\n{n_null_movie_metadata}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_metadata_path = os.path.join(\"data\",\"CMU\",\"character.metadata.tsv\")\n",
    "character_metadata = pd.read_csv(character_metadata_path, delimiter=\"\\t\", header = None)\n",
    "character_metadata.columns = [\n",
    "    'Wikipedia_movie_ID', \n",
    "    'Freebase_movie_ID', \n",
    "    'Movie_release_date', \n",
    "    'Character_name', \n",
    "    'Actor_date_of_birth', \n",
    "    'Actor_gender', \n",
    "    'Actor_height', \n",
    "    'Actor_ethnicity', \n",
    "    'Actor_name', \n",
    "    'Actor_age_at_movie_release', \n",
    "    'Freebase_character_actor_map_ID', \n",
    "    'Freebase_character_ID', \n",
    "    'Freebase_actor_ID'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_metadata['Actor_name']= character_metadata['Actor_name'].fillna('unknown')\n",
    "character_metadata[\"Actor_name\"] = character_metadata[\"Actor_name\"].str.lower()\n",
    "\n",
    "# Ensures that the date columns are correctly interpreted as datetime objects in pandas. \n",
    "character_metadata['Movie_release_date'] = pd.to_datetime(character_metadata['Movie_release_date'], errors='coerce').dt.date\n",
    "character_metadata['Actor_date_of_birth'] = pd.to_datetime(character_metadata['Actor_date_of_birth'], errors='coerce').dt.date\n",
    "\n",
    "# Drop rows where 'Movie_release_date' or 'Actor_date_of_birth' is NaT\n",
    "character_metadata.dropna(subset=['Movie_release_date', 'Actor_date_of_birth'], inplace=True)\n",
    "\n",
    "# Calculate Actor_age_at_movie_release if missing and data is available\n",
    "character_metadata['Actor_age_at_movie_release'] = character_metadata.apply(\n",
    "    lambda row: (row['Movie_release_date'] - row['Actor_date_of_birth']).days // 365 if pd.isnull(row['Actor_age_at_movie_release']) else row['Actor_age_at_movie_release'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#Drop unnecessary columns , heere the actors' heights\n",
    "character_metadata.drop(columns=['Actor_height', 'Actor_ethnicity', 'Character_name'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Movies without actor gender data: {character_metadata['Actor_gender'].isnull().sum()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore drop the mvies with no information the gender of the actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_metadata.dropna(subset=['Actor_gender'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's groupe the actors together by creating a *Cast* column on the *movie_metadata_df*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_per_movie = character_metadata.groupby('Freebase_movie_ID')['Actor_name'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "cast_per_movie = cast_per_movie.rename(columns={'Actor_name': 'Cast'})\n",
    "\n",
    "movie_metadata_df = movie_metadata_df.merge(cast_per_movie, on='Freebase_movie_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cast_count = movie_metadata_df[\"Cast\"].isna().sum()\n",
    "total_movies = len(movie_metadata_df)\n",
    "print(f\"Movies without actor data: {missing_cast_count} out of {total_movies} ({missing_cast_count / total_movies * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to drop the movies without actor data since it is one of the most important data of ur study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_metadata_df = movie_metadata_df.dropna(subset = ['Cast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for male, female actor counts\n",
    "actor_counts = character_metadata.groupby('Freebase_movie_ID')['Actor_gender'].value_counts().unstack(fill_value=0)\n",
    "actor_counts = actor_counts.rename(columns={'M': 'Male_actors', 'F': 'Female_actors'})\n",
    "\n",
    "# Ensure all columns are present (set missing columns to 0 if necessary)\n",
    "for col in ['Male_actors', 'Female_actors']:\n",
    "    if col not in actor_counts.columns:\n",
    "        actor_counts[col] = 0\n",
    "\n",
    "actor_counts = actor_counts.reset_index()\n",
    "\n",
    "# Merge actor counts with the movie_metadata_df DataFrame\n",
    "movie_metadata_df = movie_metadata_df.merge(actor_counts, on='Freebase_movie_ID', how='left')\n",
    "\n",
    "# Fill NaN values with 0 \n",
    "movie_metadata_df[['Male_actors', 'Female_actors']] = movie_metadata_df[['Male_actors', 'Female_actors']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of female actors in each movie and round to two decimal places\n",
    "movie_metadata_df['Female_actor_percentage'] = (\n",
    "    (movie_metadata_df['Female_actors'] / \n",
    "     (movie_metadata_df['Female_actors'] + movie_metadata_df['Male_actors'])) * 100\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use two IMDB datasets to decribe movies:\n",
    "1. \"title.ratings.tsv\" gives us the ratings of the movies as voted by viewers. \n",
    "2. \"title.basics.tsv\", indexes into \"title.ratings.tsv\" using a alphanumeric unique identifier of the title. It gives general information about the movie such as runtime, release date and adult rating.\n",
    "3. \"title.crew.tsv\", indexes into the previous two using the same alphanumeric unique identifier of the title. It gives information on the directors and writers of the movie.\n",
    "\n",
    "Reference:\n",
    "Internet Movie Database. (2024). IMDb non-commercial datasets. Retrieved from https://developer.imdb.com/non-commercial-datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets. Null values are represented using \"\\N\".\n",
    "IMDB_ratings_path = os.path.join(\"data\",\"IMDB\",\"title.ratings.tsv\")\n",
    "IMDB_ratings_df = pd.read_csv(IMDB_ratings_path, delimiter=\"\\t\", na_values=\"\\\\N\", names=[\"tconst\", \"Average_ratings\", \"Num_votes\"], low_memory= False)\n",
    "\n",
    "IMDB_titles_path = os.path.join(\"data\",\"IMDB\",\"title.basics.tsv\")\n",
    "column_names_basics = [\"tconst\",\n",
    "                       \"Title_type\",\n",
    "                       \"Primary_title\",\n",
    "                       \"Original_title\",\n",
    "                       \"Is_adult\",\n",
    "                       \"Start_year\",\n",
    "                       \"End_year\",\n",
    "                       \"Movie_runtime\",\n",
    "                       \"Movie_genres\"]\n",
    "IMDB_basics_df = pd.read_csv(IMDB_titles_path, delimiter=\"\\t\", na_values=\"\\\\N\", low_memory=False, names=column_names_basics)\n",
    "\n",
    "IMDB_crew_path = os.path.join(\"data\",\"IMDB\",\"title.crew.tsv\")\n",
    "IMDB_crew_df = pd.read_csv(IMDB_crew_path, delimiter=\"\\t\", na_values=\"\\\\N\", low_memory=False, names=[\"tconst\", \"Directors\", \"Writers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the lengths of the datasets\n",
    "print(f\"Length of IMDB_ratings_df: {len(IMDB_ratings_df):,}\\n\"\n",
    "      f\"Length of IMDB_basics_df: {len(IMDB_basics_df):,}\\n\"\n",
    "      f\"Length of IMDB_crew_df: {len(IMDB_crew_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before dealing with the null values we will merge the dataframes together using the alphanumeric unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all three datasets.\n",
    "IMDB_merged_df = pd.merge(IMDB_ratings_df, IMDB_basics_df, how=\"inner\", left_on=\"tconst\", right_on=\"tconst\")\n",
    "IMDB_merged_df = pd.merge(IMDB_merged_df, IMDB_crew_df, how=\"inner\", on=\"tconst\")\n",
    "\n",
    "print(f\"The resulting merged dataframe has length: {len(IMDB_merged_df):,}.\")\n",
    "print(f\"{len(IMDB_ratings_df)-len(IMDB_merged_df):,} rows were lost in the merging process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we do not lose a lot of rows with respect to the IMDB_ratings_df dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at titleType. These dataframes do not only have movies but also short movies, tv shows, episodes. The next step is thus to filter only movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering movies from the list of titles.\n",
    "IMDB_merged_df = IMDB_merged_df[IMDB_merged_df[\"Title_type\"] == \"movie\"]\n",
    "\n",
    "print(f\"There are {len(IMDB_merged_df):,} movies in the resulting dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the strings appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_merged_df[\"Original_title\"] = IMDB_merged_df[\"Original_title\"].apply(clean_string_list)\n",
    "IMDB_merged_df[\"Primary_title\"] = IMDB_merged_df[\"Primary_title\"].apply(clean_string_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at null values in the merged IMDB dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the percentage of null values in the dataset.\n",
    "n_null_IMDB = ((IMDB_merged_df.isnull().sum() / len(IMDB_merged_df)) * 100).apply(lambda x: f\"{x:,.2f}%\")\n",
    "\n",
    "print(\n",
    "    f\"Percentage of null values per column:\\n\"\n",
    "    f\"IMDB_ratings_df:\\n{n_null_IMDB}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end year is always missing. Other than that the proportion of missing values is very small (< 12%). End year does not have any useful information for our intended analysis and can thus be dropped. We can also drop the titleType column since we know they are all movies after the filtering that was done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns.\n",
    "IMDB_merged_df = IMDB_merged_df.drop(columns=[\"End_year\", \"Title_type\"], axis=1)\n",
    "\n",
    "print(f\"The resulting dataframe has {len(IMDB_merged_df):,} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ratings from the IMDB dataset during our study. However, these ratings are based on viewer votes. Initially we thought of discarding rows with too few votes. However, there could be a link between number of votes and number of views of a movie (although definitely not a direct one). We thus decided to keep all rows for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Merging the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.1 Merging IMDB and CMU Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging on the original title.\n",
    "merge1 = pd.merge(IMDB_merged_df, movie_metadata_df, how=\"inner\", left_on=\"Original_title\", right_on=\"Movie_name\")\n",
    "# Merging on the primary title.\n",
    "merge2 = pd.merge(IMDB_merged_df, movie_metadata_df, how=\"inner\", left_on=\"Primary_title\", right_on=\"Movie_name\")\n",
    "\n",
    "# Concatenating and dropping duplicates that appear from movies with the same Original_title and Primary_title.\n",
    "movie_df = pd.concat([merge1, merge2]).drop_duplicates(subset=[\"tconst\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"The resulting dataframe has {len(movie_df):,} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns appear twice. Let's take a look at the proportion of null values in each duplicate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_null_movie = (movie_df.isnull().sum()/len(movie_df)).apply(lambda x:f\"{x:.2%}\")\n",
    "\n",
    "print(f\"Percentage of null values per column:\\n{n_null_movie}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see:\n",
    "- Movie_runtime_x and Movie_runtime_y have 6.02% and 11.58% missing values respectively. We will combine the non null values from both these columns into a new column called runtimeMinutes and then drop the previous two columns. \n",
    "- Movie_genres_x has 1.54% missing values against 0.00% missing values for Movie_genres_y. Furthermore Movie_genres_y is from the CMU dataset and seems more complete. We will thus drop the genres_x column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all non-null values from Movie_runtime_x and Movie_runtime_y into runtimeMinutes.\n",
    "movie_df[\"Movie_runtime\"] = movie_df[\"Movie_runtime_x\"].combine_first(movie_df[\"Movie_runtime_y\"])\n",
    "\n",
    "# Dropping the unnecessary columns.\n",
    "movie_df.drop(columns=[\"Movie_runtime_x\", \"Movie_runtime_y\", \"Movie_genres_x\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see if Movie_name, originalTitle and primaryTitle are all necessary or if there are any redundancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = movie_df[\"Movie_name\"] == movie_df[\"Primary_title\"]  \n",
    "test2 = movie_df[\"Movie_name\"] == movie_df[\"Original_title\"]\n",
    "\n",
    "# Checking if there are any movies for which Movie_name is not either in Primary_title or Original_title\n",
    "print(f\"There are {(~(test1 | test2)).sum().item()} movies for which Movie_name is in neither Primary_title ot Original_title.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the Movie_name column is redundant as its information is either in primaryTitle or in originalTitle. We can thus drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.drop(columns=\"Movie_name\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also drop movie identifier columns (as everything is already indexed): tconst and Freebase_movie_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.drop(columns=[\"tconst\", \"Freebase_movie_ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect this with the plot_summaries_df, we perform a left merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.merge(movie_df, plot_summaries_df, on=\"Wikipedia_movie_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can drop Wikipedia_movie_ID, which is also a movie identifier column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.drop(columns=\"Wikipedia_movie_ID\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.3 Merging Kaggle and The numbers datasets to the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem, a lot of missing values(NaN or 0) in Movie box office revenue in movie_df and no production budget included. \n",
    "These two new datasets include information about movies revenue and budget and other attributes which are crucial for our analysis.\n",
    "\n",
    "- `Kaggle_df`: Contains important details about movies, including budgets, genres, and revenue.\n",
    "- `TheNumbers_df`: Provides detailed financial data, focusing on budget and box office gross both domestically and worldwide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggle_path = os.path.join(\"data\",\"KaggleSingh\",\"movie_dataset.csv\")\n",
    "Kaggle_df = pd.read_csv(Kaggle_path, na_values=\"\\\\N\", low_memory=False)\n",
    "\n",
    "TheNumbers_path = os.path.join(\"data\",\"TheNumbers\",\"tn.movie_budgets.csv\")\n",
    "TheNumbers_df = pd.read_csv(TheNumbers_path, na_values=\"\\\\N\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly view the structure of the dataframes to understand the data types and confirm data has loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TheNumbers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now reduce the complexity of the Kaggle dataset by retaining only the columns necessary for our analysis. Additionally, we clean and convert financials of both datasets to float for consistency with the movie_df. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unnecessary columns.\n",
    "Kaggle_df = Kaggle_df[[\"budget\", \"genres\", \"original_title\", \"popularity\", \"production_companies\", \"production_countries\", \"revenue\"]]\n",
    "Kaggle_df[\"budget\"] = Kaggle_df[\"budget\"].astype(float)\n",
    "Kaggle_df[\"revenue\"] = Kaggle_df[\"revenue\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to remove '$' and ',' from the financials\n",
    "def clean_currency_column(column):\n",
    "    return column.str.replace('[^\\d.]', '', regex=True).astype(float)\n",
    "\n",
    "TheNumbers_df[\"production_budget\"] = clean_currency_column(TheNumbers_df[\"production_budget\"])\n",
    "TheNumbers_df[\"domestic_gross\"] = clean_currency_column(TheNumbers_df[\"domestic_gross\"])\n",
    "TheNumbers_df[\"worldwide_gross\"] = clean_currency_column(TheNumbers_df[\"worldwide_gross\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TheNumbers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform an outer merge to include all records from both datasets, aligning them based on movie titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the title columns for case insensitive matching\n",
    "Kaggle_df[\"original_title\"] = Kaggle_df[\"original_title\"].str.strip().str.lower()\n",
    "TheNumbers_df[\"movie\"] = TheNumbers_df[\"movie\"].str.strip().str.lower()\n",
    "\n",
    "# Merge both dataframes on the title columns\n",
    "merged_df = pd.merge(Kaggle_df, TheNumbers_df, left_on=\"original_title\", right_on=\"movie\", how='outer')\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-merge cleaning\n",
    "\n",
    "We create the main financial columns ('movie_budget' and 'movie_revenue') by selecting non-null and non-zero values, prioritizing data from TheNumbers dataset when available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['movie_budget'] = np.where((merged_df['production_budget'].notna() & (merged_df['production_budget'] != 0)),\n",
    "                                     merged_df['production_budget'],\n",
    "                                     merged_df['budget'])\n",
    "\n",
    "merged_df['movie_revenue'] = np.where((merged_df['worldwide_gross'].notna() & (merged_df['worldwide_gross'] != 0)),\n",
    "                                      merged_df['worldwide_gross'],\n",
    "                                      merged_df['revenue'])\n",
    "\n",
    "# Drop the original columns that were merged into new ones\n",
    "merged_df.drop(columns=['budget', 'revenue', 'production_budget', 'worldwide_gross', 'movie'], inplace=True)\n",
    "\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out movies where both budget and revenue data are missing or zero, as they do not provide value for financial analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask where both columns are either zero or NaN\n",
    "mask = (\n",
    "    (merged_df[\"movie_budget\"].isna() | (merged_df[\"movie_budget\"] == 0)) &\n",
    "    (merged_df[\"movie_revenue\"].isna() | (merged_df[\"movie_revenue\"] == 0))\n",
    ")\n",
    "\n",
    "# Apply the mask and drop those rows\n",
    "df_filtered = merged_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of available and relevant movie budget information is \" + str(df_filtered[\"movie_budget\"].size))\n",
    "print(\"The number of available and relevant movie revenue information is \" + str(df_filtered[\"movie_revenue\"].size))\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two merging strategies are employed here to maximize the potential for matching records across the datasets:\n",
    "\n",
    "Original Title Merge: Merging based on the Original_title from movie_df to the original_title from df_filtered to catch the most direct title matches. \n",
    "\n",
    "Secondary Title Merge: Using Primary_title as an alternative matching criterion, knowing that some movies might be listed under alternate titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the title columns for case-insensitive matching\n",
    "movie_df[\"Original_title\"] = movie_df[\"Original_title\"].str.strip().str.lower()\n",
    "movie_df[\"Primary_title\"] = movie_df[\"Primary_title\"].str.strip().str.lower()\n",
    "\n",
    "# Merging on the original title.\n",
    "merge1 = pd.merge(movie_df, df_filtered, how=\"left\", left_on=\"Original_title\", right_on=\"original_title\")\n",
    "# Merging on the secondary title.\n",
    "merge2 = pd.merge(movie_df, df_filtered, how=\"left\", left_on=\"Primary_title\", right_on=\"original_title\")\n",
    "\n",
    "# Concatenating and dropping duplicates that appear from movies with the same Original_title and Primary_title.\n",
    "final_df = pd.concat([merge1, merge2]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Filter out movies where revenue data is missing or zero and prioritize initial revenue\n",
    "final_df[\"final_movie_revenue\"] = np.where(\n",
    "    (final_df[\"Movie_box_office_revenue\"].notna() & (final_df[\"Movie_box_office_revenue\"] != 0)),\n",
    "    final_df[\"Movie_box_office_revenue\"],\n",
    "    final_df[\"movie_revenue\"]\n",
    ")\n",
    "\n",
    "final_df.drop(columns=[\"Movie_box_office_revenue\", \"movie_revenue\", \"original_title\", \"genres\", \"id\", \"release_date\"], inplace=True, errors='ignore')\n",
    "\n",
    "# Renaming the columns to follow the convention\n",
    "rename_columns = {\"popularity\":\"Popularity\", \n",
    "                  \"production_companies\":\"Production_companies\", \n",
    "                  \"prodcution_countries\":\"Production_countries\",\n",
    "                  \"domestic_gross\":\"Domestic_gross\",\n",
    "                  \"movie_budget\":\"Movie_budget\",\n",
    "                  \"final_movie_revenue\":\"Final_movie_revenue\",\n",
    "                  \"Movie_genres_y\":\"Movie_genres\"}\n",
    "\n",
    "final_df.rename(columns=rename_columns, inplace=True)\n",
    "\n",
    "\n",
    "print(f\"The resulting dataframe has {len(final_df):,} rows.\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. <u>Our success metric</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Financial Analysis\n",
    "\n",
    "##### Calculating Return on Investment (ROI)\n",
    "\n",
    "To provide insights into the financial success of the movies, we calculate the Return on Investment (ROI). This metric is derived by comparing the movie's final revenue to its budget.\n",
    "\n",
    "- **Condition**: Ensure that both `movie_budget` and `final_movie_revenue` are non-zero and non-null to avoid division errors and ensure data integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (final_df['Movie_budget'].notna() & final_df['Movie_budget'].ne(0) &\n",
    "             final_df['Final_movie_revenue'].notna() & final_df['Final_movie_revenue'].ne(0))\n",
    "\n",
    "# Apply the condition to filter the dataframe\n",
    "final_df = final_df[condition]\n",
    "print(\"There is now \" + str(final_df[\"Final_movie_revenue\"].size) + \" movies to work with in the financial success analysis\")\n",
    "\n",
    "# Calculating ROI column based on profits and budget\n",
    "final_df[\"ROI\"] = ((final_df[\"Final_movie_revenue\"] - final_df[\"Movie_budget\"]) / final_df[\"Movie_budget\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalization and Scaling\n",
    "\n",
    "Given the varied scales of ratings and ROI, we standardize these features to a common scale using both Standard and Min-Max scalers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "final_df[\"Average_ratings\"] = final_df[\"Average_ratings\"].astype(float)\n",
    "final_df[\"Normalized_Rating\"] = final_df[\"Average_ratings\"] / 10\n",
    "\n",
    "# Standardize the selected data\n",
    "final_df[\"ROI\"] = standard_scaler.fit_transform(final_df[[\"ROI\"]])\n",
    "\n",
    "# Apply Min-Max scaling to the already standardized columns\n",
    "final_df[\"ROI\"] = minmax_scaler.fit_transform(final_df[[\"ROI\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Success Metric\n",
    "\n",
    "To evaluate overall movie success, we calculate a weighted sum of normalized ratings and ROI, assigning double the weight to ROI given its financial significance in assessing success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI feature will be double the importance of ratings\n",
    "weights = {'ROI': 0.67, 'Normalized_Rating': 0.33}\n",
    "\n",
    "# Calculate weighted sum\n",
    "final_df['movie_success'] = final_df['ROI'] * weights['ROI'] + final_df['Normalized_Rating'] * weights['Normalized_Rating']\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <u>Gender and age vs success</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have have established our success metric, we can evaluate the effect of actors' gender on the success of a movie. \n",
    "\n",
    "The best way we chose in order to conduct our analysis is a **Regression analysis**. Indeed, this will allow us to quantify and understand the relationship between gender, age, and other factors on ROI, ratings and our success metric while controlling for potential confounders.\n",
    "\n",
    "Before diving into regression, we should perform an **Exploratory Data Analysis (EDA)** to understand the relationships among variables and detect any anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Exploratory Data Analysis (EDA)\n",
    "\n",
    "First, let's list the variable of interest:\n",
    "\n",
    "Categorical variable:\n",
    "- *Actor_gender*: An actor is either Male or Female. \n",
    "- *isAdult*: A boolean. Returns 1 if it a +18 rated movie\n",
    "- *Movie_genre*\n",
    "- *Movie_countries*, \n",
    "- *Movie_languages*\n",
    "\n",
    "Continous variable:\n",
    "- *Average_female_actor_age*: Represents the average age of the female actresses. \n",
    "- *Average_male_actor_age*: Represents the average age of the male actresses.\n",
    "- *Average_ratings* and *Movie_box_office_revenue*: These are the dependent variables of our studies.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A quick comment about the dependent variables:\n",
    "The goal of our study is to look at ratings, ROI and our succes metric. However, due to complications with finding enough data for the ROI and success metric estimation we will need to postpone the in-depth analysis to P3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.1 Univariate Analysis of continuous variables\n",
    "a. <u>Numerical Analysis</u>: \n",
    "\n",
    "As a numerical analysis, we can show a table with tne minimum, maximum, mean, standar deviation (SD) and median of each variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_copy = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_copy = final_df_copy[final_df_copy[\"Movie_languages\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\"Average_ratings\",\n",
    "                   \"Num_votes\",\n",
    "                   \"Movie_release_date\",\n",
    "                   \"Final_movie_revenue\",\n",
    "                   \"Movie_runtime\",\n",
    "                   \"Female_actors\",\n",
    "                   \"Male_actors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "for col in numeric_columns:\n",
    "    final_df_copy[col] = pd.to_numeric(final_df_copy[col], errors=\"coerce\")\n",
    "\n",
    "final_df_copy.dropna(subset=numeric_columns, inplace=True)\n",
    "\n",
    "# Summary statistics\n",
    "summary_table = final_df_copy[numeric_columns].agg([\"min\", \"max\", \"mean\", \"std\", \"median\"]).T\n",
    "summary_table.columns = [\"Min\", \"Max\", \"Mean\", \"SD\", \"Median\"]\n",
    "summary_table = summary_table.round(2)\n",
    "\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 6))\n",
    "fig.suptitle(\"Figure 1: Histogram of Dependent Variables\", fontsize=14)\n",
    "axes = axes.flatten()\n",
    "\n",
    "dep_var = [\"Average_ratings\", \"Final_movie_revenue\", \"ROI\"]\n",
    "\n",
    "# Plot histograms for each dependent variables\n",
    "for i, col in enumerate(dep_var):\n",
    "    if i < len(axes):  # Ensure we do not exceed the number of axes\n",
    "        ax = axes[i]\n",
    "        sns.histplot(final_df_copy, x=col, kde=True, stat=\"density\", ax=ax, bins=15)\n",
    "        ax.set_title(f\"Histogram of {col}\")\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "# Hide any extra subplots if there are more axes than columns\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Make space for the title\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P2, we will ignore the ROI metric and look at Average_ratings and Final_movie_revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in Figure 1, the distribution of *Movie_box_office_revenue* is highly skewed. In order to adresse that, we have to apply a log transformation to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(final_df_copy['Final_movie_revenue'].apply(math.log1p), kde=True, edgecolor=\"black\", bins=15)\n",
    "plt.title(\"Histogram of log-transformed Final_movie_revenue\")\n",
    "plt.xlabel(\"log_Movie_box_office_revenue\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully obtained a normal distribution for the *Movie_box_office_revenue*. Now, let's plot the independent variables. \n",
    "\n",
    "Let's now look at our independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as for the dependent variables\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(14, 8))\n",
    "fig.suptitle(\"Figure 2: Histogram of Independent Variables\", fontsize=14)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "indep_var = [\"Num_votes\", \"Movie_release_date\", \"Movie_runtime\", \"Female_actors\", \"Male_actors\"]\n",
    "\n",
    "# Plot histograms for each independent variable\n",
    "for i, col in enumerate(indep_var):\n",
    "    sns.histplot(final_df_copy, x=col, kde=True, stat=\"density\", ax=axes[i], bins=15)\n",
    "    axes[i].set_title(f\"Histogram of {col}\")\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to skewed independent variables\n",
    "final_df_copy['log_Num_votes'] = final_df_copy['Num_votes'].apply(math.log1p)\n",
    "final_df_copy['log_Movie_runtime'] = final_df_copy['Movie_runtime'].apply(math.log1p)\n",
    "final_df_copy['log_Female_actors'] = final_df_copy['Female_actors'].apply(math.log1p)\n",
    "final_df_copy['log_Male_actors'] = final_df_copy['Male_actors'].apply(math.log1p)\n",
    "\n",
    "# Plot histograms of the transformed variables\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(18, 6))\n",
    "fig.suptitle(\"Figure: Histograms of Log-Transformed Independent Variables\", fontsize=14)\n",
    "\n",
    "sns.histplot(final_df_copy['log_Num_votes'], kde=True, ax=axes[0], color=\"skyblue\", bins=15)\n",
    "axes[0].set_title(\"Log of Num_votes\")\n",
    "\n",
    "sns.histplot(final_df_copy['log_Movie_runtime'], kde=True, ax=axes[1], color=\"skyblue\", bins=15)\n",
    "axes[1].set_title(\"Log of Movie_runtime\")\n",
    "\n",
    "sns.histplot(final_df_copy['log_Female_actors'], kde=True, ax=axes[2], color=\"skyblue\", bins=15)\n",
    "axes[2].set_title(\"Log of Female_actors\")\n",
    "\n",
    "sns.histplot(final_df_copy['log_Male_actors'], kde=True, ax=axes[3], color=\"skyblue\", bins=15)\n",
    "axes[3].set_title(\"Log of Male_actors\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "for i, col in enumerate(numeric_columns, 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "    sns.boxplot(data=final_df_copy, x=col)\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "    \n",
    "plt.suptitle(\"Figure 3: Boxplot of Variables\")     \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to focus on outliers and removing them for a more accurate modeling. From the boxplots, we chose the remove the outliers of the *Movie_box_office_revenue* and the *Minutes_runtime* variables. To do that, we use the **InterQuartile Range (IQR) method**, which identifies values that are significantly higher or lower than the majority of the data.\n",
    "\n",
    "The *remove_outliers* function will calculate the IQR for each column specified and removes any rows with values outside the range:\n",
    "\n",
    "$[Q1 - 1.5 \\times \\text{IQR}, Q3 + 1.5 \\times \\text{IQR}]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove outliers based on IQR\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        # Define lower and upper bounds for outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Remove rows with values outside these bounds\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Create a copy of the dataframe without outliers\n",
    "final_df_no_outliers = remove_outliers(final_df_copy, numeric_columns)\n",
    "\n",
    "# Display the boxplot again to verify the outliers are removed\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(numeric_columns, 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "    sns.boxplot(data=final_df_no_outliers, x=col)\n",
    "    plt.title(f\"Boxplot of {col} (Without Outliers)\")\n",
    "plt.suptitle(\"Figure 4: Boxplot of Variables without Outliers\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After removing the outliers and dropping the null values, we are left with {len(final_df_no_outliers):,} movies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.2 Bivariate Analysis of continuous variables\n",
    "a. <u>Correlation matrix</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = final_df_no_outliers[numeric_columns].corr()\n",
    "\n",
    "# Calculate and visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Figure 5: Correlation Matrix of Numeric Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix for the continuous variables in our dataset shows the following relationships:\n",
    "\n",
    "•⁠  ⁠Female_actors and Male_actors : The correlation coefficient is *0.19*, indicating a weak positive correlation. This suggests that movies with a higher number of female actors also tend to have a higher number of male actors. We note that this could indicate a general tendency for larger casts to include more actors of both genders.\n",
    "\n",
    "•⁠  ⁠Final_movie_revenue and Male_actors : The correlation coefficient is *0.26*, indicating a moderate positive correlation. This suggests that movies with more male actors might achieve higher box office earnings. \n",
    "\n",
    "•⁠  ⁠Final_movie_revenue and Female_actors : The correlation coefficient is *0.16*, indicating a weak positive correlation. This suggests that movies with more female actors also have a slight tendency to generate higher revenue. We note however that this relationship is weaker than with male actors. \n",
    "\n",
    "•⁠  ⁠Female_actors and Movie_release_date : The correlation coefficient is *0.15*, indicating a weak positive correlation. This suggests that more recent movies tend to include a higher number of female actors. \n",
    "\n",
    "While gender composition affects the correlation, these findings highlight the need for deeper analysis (e.g., regression) to control for potential confounders and isolate the impact of gender on box office success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. <u>Scatter plots </u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(final_df_no_outliers[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The categorical variables\n",
    "Regression analysis with many categorical variables can quickly get complicated. Indeed using a boolean value for each category except one can cause us to have too many regressors. For variables like isAdult and Gender it is easy as there are only two possibilities. However, for Movie_genre, Movie_countries and Movie_languages we can quickly get into the hundreds. Here is how we plan on managing this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a cutoff threshold of 1000. This means all genres with less than 1000 occurences will be discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # Threshold for filtering genres\n",
    "\n",
    "genres_series = (\n",
    "    final_df_copy[\"Movie_genres\"]\n",
    "    .str.replace(r\"[\\[\\]']\", \"\", regex=True)  # Clean the string further for this application\n",
    "    .str.split(', ')  # Split the genre strings into lists for easier manipulation, using ', ' as the delimiter\n",
    "    .explode()  # Expand the lists into separate rows with one genre per row\n",
    "    .str.strip()  # Strip leading and trailing whitespace from each genre\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# Get the count of each genre\n",
    "genre_counts = genres_series.value_counts()\n",
    "\n",
    "# Filter out genres with counts < N\n",
    "filtered_genre_counts = genre_counts[genre_counts >= N]  \n",
    "\n",
    "# Count the number of genres appearing at least N times\n",
    "len(filtered_genre_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if this restricts our study to too few movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_genre_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of genres that meet the threshold\n",
    "popular_genres = filtered_genre_counts.index.tolist()\n",
    "\n",
    "# Remove all special characters from the strings to allow proper regex matching\n",
    "no_special_char = map(re.escape, popular_genres)\n",
    "\n",
    "# Create regex pattern using the \"OR\" operator\n",
    "pattern_genre = '|'.join(no_special_char) \n",
    "\n",
    "# Check thanks to the regex pattern if any of the popular genres are in each column of th edataframe\n",
    "rows_with_popular_genres = final_df_copy[\"Movie_genres\"].str.contains(pattern_genre, case=False, na=False)\n",
    "\n",
    "print(f\"There are {rows_with_popular_genres.sum()} movies that belong to at least one of the {len(filtered_genre_counts)} most popular genres.\",\n",
    "      f\"\\nThere were {len(final_df_copy)} movies in the dataset before this operation,\", \n",
    "      f\"meaning we lost {(len(final_df_copy) - rows_with_popular_genres.sum())/len(final_df_copy):.2%} of the movies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the movies that have none of the most popular genres. We can also assume that the genres are ordered by relevance. We will thus keep for each movie, the first popular genre that appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_genre(genres, popular_genres):\n",
    "    for genre in map(str.strip, genres.split(\",\")): # Split genres according to \",\" then remove spaces\n",
    "        if genre.lower() in popular_genres: # We are going through the list of genres in the order it is presented\n",
    "            return genre # Return the first genre that is in popular_genres\n",
    "    \n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_copy = final_df_copy[rows_with_popular_genres]\n",
    "\n",
    "# Ensuring the popular_genres are lowercase\n",
    "popular_genres = set(map(str.lower, popular_genres))\n",
    "\n",
    "# Apply the function to the dataset and store the results in the Main_genre column\n",
    "final_df_copy[\"Main_genre\"] = final_df_copy[\"Movie_genres\"].apply(lambda x : get_main_genre(x, popular_genres)).str.replace(\" \",\"_\")\n",
    "\n",
    "# Plotting the distribution of main genres\n",
    "genre_counts = final_df_copy[\"Main_genre\"].value_counts().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(genre_counts.index, genre_counts.values, edgecolor='black')\n",
    "plt.title(\"Distribution of Main Genres\", fontsize=16)\n",
    "plt.xlabel(\"Main Genres\", fontsize=14)\n",
    "plt.ylabel(\"Number of Movies\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.grid(axis=\"y\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_copy = pd.get_dummies(final_df_copy, columns=[\"Main_genre\"], drop_first=True) # Reference category is action\n",
    "final_df_copy.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_series = (\n",
    "    final_df_copy['Movie_languages']\n",
    "    .str.replace(r\"[\\[\\]']\", \"\", regex=True) \n",
    "    .str.split(', ')\n",
    "    .explode()\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" language\", \"\")\n",
    ")\n",
    "\n",
    "\n",
    "language_count = language_series.value_counts()\n",
    "language_count.head(10).plot(kind='bar', figsize=(10, 6))\n",
    "plt.xlabel('Languages')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.title('Top 10 Languages by Occurrences')\n",
    "plt.xticks(rotation=0)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Movie_languages we decided to take the 4 most common languages. This allows us to analyze the impact of language. However, we will still have an unbalanced dataset with English being much more prominent than other languages.\n",
    "\n",
    "We will conduct a first analysis using only English, then we will conduct a second analysis with the top 4 languages after rebalancing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_languages = language_count.index.tolist()\n",
    "\n",
    "# Dropping all the rows that do not have one of the top 4 languages.\n",
    "final_df_copy = final_df_copy[final_df_copy['Movie_languages'].str.contains('|'.join(top_languages), case=False, na=False)]\n",
    "\n",
    "print(f\"We are left with {len(final_df_copy)} datapoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again have certain rows with multiple languages. We must proceed in a similar way to the genres above to obtain the dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing movies into two groups: \n",
    "- those available exclusively in English, \n",
    "- those available in other languages (which may include English alongside another language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving corresponding labels to each row 'English Only' or 'Not Only English'\n",
    "final_df_copy['Language_Category'] = final_df_copy['Movie_languages'].apply(\n",
    "    lambda x: 'English_only' if x.lower() == 'english language' else 'Not_only_english'\n",
    ")\n",
    "\n",
    "# Creating the dummy variable and dropping the English_only category to be used as the reference category\n",
    "final_df_copy = pd.get_dummies(final_df_copy, columns=[\"Language_Category\"], drop_first=True)\n",
    "\n",
    "# Renaming the column.\n",
    "final_df_copy.rename(columns={'Language_Category_Not_only_english': 'Is_not_only_english'}, inplace=True)\n",
    "\n",
    "# Counting the number of movies available in other languages\n",
    "not_only_eng_count = final_df_copy[\"Is_not_only_english\"].sum()\n",
    "\n",
    "print(f\"There are {not_only_eng_count} movies that are available in other languages and {len(final_df_copy)-not_only_eng_count} movies that are only available in English.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD METHOD (KEPT FOR REFERENCE)\n",
    "\n",
    "We can see how unbalanced the dataset is, with 5840 english movies against around 400 movies for the other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Identifying Potential Confounders\n",
    "In order to not have bias in our study, it is essential we add the confounders. This means that we have to identify the variables that have an impact on both treatment and outcome, here being the actor gender and the final box office revenue respectivly. The confounders are: \n",
    "-  ⁠Movie_genre\n",
    "-  ⁠Movie_countries\n",
    "- ⁠Movie_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. <u>How does it compare to streaming platforms? Are movies made for these platforms different?</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we want to expand our study by analyzing data from various streaming platforms. We focus on the movies of Netflix, Amazon Prime, Disney+, and Hulu. To achieve this, we created a dataset by merging data from multiple sources found on Kaggle.\n",
    "\n",
    "\n",
    "This dataset structure enables comprehensive analysis across various dimensions such as gender repartition in the cast, the ratings and description. Using these columns, we can investigate how streaming platform movies differ in content and ratings compared to traditional box office movies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.streaming import get_streaming_dataframe\n",
    "\n",
    "streaming_df = get_streaming_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. <u>Numerical Analysis</u>: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns_streaming = [\"Average_Rating\", \"Num_Votes\", \"Movie_release_date\", \"Movie_runtime\", \"Is_Adult\", \"Female_actors\", \"Male_actors\"]\n",
    "streaming_df_copy = streaming_df.copy()\n",
    "streaming_df_copy.head()\n",
    "streaming_df_copy = streaming_df_copy[streaming_df_copy['Movie_runtime'] != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#streaming_df_copy['duration'] = streaming_df_copy['duration'].str.replace(' min', '', regex=False)\n",
    "\n",
    "streaming_df_copy['Movie_runtime'] = pd.to_numeric(streaming_df_copy['Movie_runtime'], errors=\"coerce\")\n",
    "summary_table = streaming_df_copy[numeric_columns_streaming].dropna().agg([\"min\", \"max\", \"mean\", \"std\", \"median\"]).T\n",
    "summary_table.columns = [\"Min\", \"Max\", \"Mean\", \"SD\", \"Median\"]\n",
    "summary_table = summary_table.round(2)\n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. <u>Graphical Analysis</u>: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "sns.histplot(streaming_df_copy, x=\"Average_Rating\", kde=True, stat=\"density\", ax=ax, bins=15)\n",
    "ax.set_title(\"Histogram of averageRating\")\n",
    "ax.set_xlabel(\"Average_Rating\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The streaming ratings have a more balanced distribution than the movie dataframe. This contrast highlights different success metrics: streaming platforms rely on user ratings for a measure of success and box office movies are judged on revenue.\n",
    "It's more relevant to study the success of a streaming movie based on ratings, we dont need necessarly to have the revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(14, 8))\n",
    "fig.suptitle(\"Figure 2: Histogram of Independent Variables\", fontsize=14)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "indep_var = [\"Num_Votes\", \"Movie_release_date\", \"Movie_runtime\", \"Is_Adult\", \"Female_actors\", \"Male_actors\"]\n",
    "\n",
    "#Plot histograms for each independent variable\n",
    "for i, col in enumerate(indep_var):\n",
    "    sns.histplot(streaming_df_copy, x=col, kde=True, stat=\"density\", ax=axes[i], bins=15)\n",
    "    axes[i].set_title(f\"Histogram of {col}\")\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Release_year : The majority of movies in the dataset are very recent with an increase after the year 2000.\n",
    "Duration: The duration of most movies is between 90 and 120 minutes, which is typical for films.\n",
    "Is_Adult: Most movies are not classified as adult content but we can see that there is an important proportion of movies classified as adult content.\n",
    "Female_count and male_count: We can see that the number of men is greater than women but this difference is lower than for CMU/IMDB movies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "for i, col in enumerate(numeric_columns_streaming, 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "    sns.boxplot(data=streaming_df_copy, x=col)\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "    \n",
    "plt.suptitle(\"Figure 3: Boxplot of Variables\")     \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the previous part, we now have to focus on outliers and removing them for a more accurate modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove outliers based on IQR\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "\n",
    "movie_df_no_outliers = remove_outliers(streaming_df_copy, numeric_columns_streaming)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(numeric_columns_streaming, 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "    sns.boxplot(data=movie_df_no_outliers, x=col)\n",
    "    plt.title(f\"Boxplot of {col} (Without Outliers)\")\n",
    "plt.suptitle(\"Figure 4: Boxplot of Variables without Outliers\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After removing the outliers and dropping the null values, we are left with {len(movie_df_no_outliers):,} movies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.2 Bivariate Analysis\n",
    "a. <u>Correlation matrix</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "correlation_matrix = movie_df_no_outliers[numeric_columns_streaming].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Figure 5: Correlation Matrix of Numeric Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of votes and gender composition (especially the number of male actors) seem to influence movie duration. It suggests that popularity and cast composition could impact movie production characteristics.\n",
    "Weak correlations between movie duration and other variables indicate that duration might not strongly determine popularity based on votes.\n",
    "This correlation matrix suggests further analysis like regression that could help separate and clarify the effects of gender and duration on popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. <u>Scatter plots </u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(movie_df_no_outliers[numeric_columns_streaming])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. <u>What are the social reasons behind the presence of female characters in movies? Is it due to sexualization or genuine equality of representation?</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of our analysis uses plot summaries to examine whether female characters are portrayed as having agency (taking action and driving the story) or are reduced to sexualized or objectified roles. So far, we have used LLMs, such as ChatGPT Plus, to classify plot summaries based on these criteria (contains/does not contain sexualization of females), which provided useful initial results and validated the approach. However, due to time constraints during milestone 2, we were unable to use more advanced methods, such as zero-shot or few-shot learning with models like OpenAI's GPT-4, T5, or BERT variants, which we plan to explore further in milestone 3.\n",
    "The second part of our analysis focuses on adult-rated films in the resulting merged dataframe, specifically examining the gender of the actors to understand patterns of representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
