





import pandas as pd
import matplotlib.pyplot as plt
import re
import numpy as np
import seaborn as sns
import math
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import plotly.express as px
import plotly.graph_objects as go
import warnings
import transformers

from src.data.dataloader import (
    CMUDatasetLoader, IMDBDatasetLoader, KaggleDatasetLoader, NumbersDatasetLoader, BechdelDatasetLoader, DialogueDatasetLoader, ZeroShotResultsLoader, ClusteringResultsLoader)
from src.utils.EDA_utils import EDA
from src.scripts.streaming import get_streaming_dataframe
import src.models.regression as regression
import src.utils.data_utils as data_utils
import src.utils.evaluation_utils as evaluation_utils

# Suppress specific FutureWarnings about is_sparse being deprecated
warnings.filterwarnings(action="ignore", category=FutureWarning, module="sklearn", message=".*is_sparse is deprecated.*")











directory = "data"
CMU_loader = CMUDatasetLoader(directory)
plot_summaries_df = CMU_loader.load_plot_summaries()

print(f"The plot summaries dataframe has {len(plot_summaries_df):,} values.")


null_id = plot_summaries_df["Wikipedia_movie_ID"].isnull().sum()
empty_id = plot_summaries_df[plot_summaries_df["Wikipedia_movie_ID"] == ""]["Wikipedia_movie_ID"].sum()
null_plot = plot_summaries_df["Plot Summaries"].isnull().sum()
empty_plot = plot_summaries_df[plot_summaries_df["Plot Summaries"] == ""]["Plot Summaries"].sum()

print(f"Number of null values in 'Wikipedia_movie_ID': {null_id}")
print(f"Number of empty values in 'Wikipedia_movie_ID': {empty_id}")
print(f"Number of null values in 'Plot Summaries': {null_plot}")
print(f"Number of empty values in 'Plot Summaries': {empty_plot}")


movie_metadata_df = CMU_loader.load_movie_metadata()
print(f"The movie metadata dataframe has {len(movie_metadata_df):,} values.")


movie_metadata_df["Movie_name"] = (movie_metadata_df["Movie_name"]
                    .str.strip()  #Remove leading/trailing whitespace
                    .str.lower()  #Convert to lowercase
                    .replace("", np.nan)  #Replace empty strings with NaN
                   )


# Movie languages, countries, and genres are formatted as a dictionary, but they are actually a string.
# We convert them to comma seperated strings. 

column_names_to_clean = ["Movie_countries", "Movie_languages", "Movie_genres"]
movie_metadata_df = data_utils.convert_dicts_to_strings(movie_metadata_df, column_names_to_clean)

# Cleaning the strings
movie_metadata_df = data_utils.clean_string_columns(movie_metadata_df, column_names_to_clean)


# There are inconsistencies in the date format: some rows have the date as year only, others as year-month, and some as year-month-day. 
# We convert them all to the year format only since the month and day are not relevant to our analysis.
movie_metadata_df, date_pattern_summary = data_utils.standardize_dates(movie_metadata_df, "Movie_release_date")
print("Pattern Summary:\n", date_pattern_summary)


# Looking at the percentage of null values in the dataset
n_null_movie_metadata = ((movie_metadata_df.isnull().sum() / len(movie_metadata_df)) * 100).apply(lambda x: f"{x:,.2f}%")

print(
    f"Percentage of null values per column:\n"
    f"movie_metadata_df:\n{n_null_movie_metadata}"
)


character_metadata_df = CMU_loader.load_character_metadata()


character_metadata_df["Actor_name"]= character_metadata_df["Actor_name"].fillna("unknown").str.lower()

# Ensures that the date columns are correctly interpreted as datetime objects in pandas. 
character_metadata_df["Movie_release_date"] = pd.to_datetime(character_metadata_df["Movie_release_date"], errors="coerce").dt.date
character_metadata_df["Actor_date_of_birth"] = pd.to_datetime(character_metadata_df["Actor_date_of_birth"], errors="coerce").dt.date

# Drop rows where "Movie_release_date" or "Actor_date_of_birth" is NaT
character_metadata_df.dropna(subset=["Movie_release_date", "Actor_date_of_birth"], inplace=True)

# Calculate Actor_age_at_movie_release if missing and data is available
character_metadata_df["Actor_age_at_movie_release"] = character_metadata_df.apply(
    lambda row: (row["Movie_release_date"] - row["Actor_date_of_birth"]).days // 365 if pd.isnull(row["Actor_age_at_movie_release"]) else row["Actor_age_at_movie_release"],
    axis=1
)


print(f"Movies without actor gender data: {character_metadata_df['Actor_gender'].isnull().sum()}") 





character_metadata_df.dropna(subset=["Actor_gender"], inplace=True)





cast_per_movie = character_metadata_df.groupby("Freebase_movie_ID")["Actor_name"].apply(lambda x: ", ".join(x)).reset_index()
cast_per_movie = cast_per_movie.rename(columns={"Actor_name": "Cast"})

movie_metadata_df = movie_metadata_df.merge(cast_per_movie, on="Freebase_movie_ID", how="left")


missing_cast_count = movie_metadata_df["Cast"].isna().sum()
total_movies = len(movie_metadata_df)
print(f"Movies without actor data: {missing_cast_count} out of {total_movies} ({missing_cast_count / total_movies * 100:.2f}%)")





movie_metadata_df = movie_metadata_df.dropna(subset = ["Cast"])


# Create columns for male, female actor counts
actor_counts = character_metadata_df.groupby("Freebase_movie_ID")["Actor_gender"].value_counts().unstack(fill_value=0)
actor_counts = actor_counts.rename(columns={"M": "Male_actors", "F": "Female_actors"})

# Ensure all columns are present (set missing columns to 0 if necessary)
for col in ["Male_actors", "Female_actors"]:
    if col not in actor_counts.columns:
        actor_counts[col] = 0

actor_counts = actor_counts.reset_index()

# Merge actor counts with the movie_metadata_df DataFrame
movie_metadata_df = movie_metadata_df.merge(actor_counts, on="Freebase_movie_ID", how="left")

# Fill NaN values with 0 
movie_metadata_df[["Male_actors", "Female_actors"]] = movie_metadata_df[["Male_actors", "Female_actors"]].fillna(0)


# Calculate the percentage of female actors in each movie and round to two decimal places
movie_metadata_df["Female_actor_percentage"] = (
    (movie_metadata_df["Female_actors"] / 
     (movie_metadata_df["Female_actors"] + movie_metadata_df["Male_actors"])) * 100
).round(2)








# Loading the IMDB datasets.
IMDB_loader = IMDBDatasetLoader(directory)
IMDB_ratings_df = IMDB_loader.load_ratings()

IMDB_basics_df = IMDB_loader.load_basics()

IMDB_crew_df = IMDB_loader.load_crew()


# Checking the lengths of the datasets
print(f"Length of IMDB_ratings_df: {len(IMDB_ratings_df):,}\n"
      f"Length of IMDB_basics_df: {len(IMDB_basics_df):,}\n"
      f"Length of IMDB_crew_df: {len(IMDB_crew_df):,}")





# Merging all three datasets.
IMDB_merged_df = pd.merge(IMDB_ratings_df, IMDB_basics_df, how="inner", left_on="tconst", right_on="tconst")
IMDB_merged_df = pd.merge(IMDB_merged_df, IMDB_crew_df, how="inner", on="tconst")


print(f"The resulting merged dataframe has length: {len(IMDB_merged_df):,}.")





IMDB_merged_df = data_utils.clean_string_columns(IMDB_merged_df, ["Original_title", "Primary_title"])





# Checking the percentage of null values in the dataset.
n_null_IMDB = ((IMDB_merged_df.isnull().sum() / len(IMDB_merged_df)) * 100).apply(lambda x: f"{x:,.2f}%")

print(
    f"Percentage of null values per column:\n"
    f"IMDB_ratings_df:\n{n_null_IMDB}"
)





# Dropping unnecessary columns.
IMDB_merged_df = IMDB_merged_df.drop(columns=["End_year", "Title_type"], axis=1)

print(f"The resulting dataframe has {len(IMDB_merged_df):,} rows.")











# Merging on the original title.
merge1 = pd.merge(IMDB_merged_df, movie_metadata_df, how="inner", left_on=["Original_title", "Start_year"],  right_on=["Movie_name", "Movie_release_date"])
# Merging on the primary title.
merge2 = pd.merge(IMDB_merged_df, movie_metadata_df, how="inner", left_on=["Primary_title", "Start_year"], right_on=["Movie_name", "Movie_release_date"])

# Concatenating and dropping duplicates that appear from movies with the same Original_title and Primary_title.
movie_df = pd.concat([merge1, merge2]).drop_duplicates(subset=["tconst"]).reset_index(drop=True)

print(f"The resulting dataframe has {len(movie_df):,} rows.")





n_null_movie = (movie_df.isnull().sum()/len(movie_df)).apply(lambda x:f"{x:.2%}")

print(f"Percentage of null values per column:\n{n_null_movie}")





# Combining all non-null values from Movie_runtime_x and Movie_runtime_y into runtimeMinutes.
movie_df["Movie_runtime"] = movie_df["Movie_runtime_x"].combine_first(movie_df["Movie_runtime_y"])

# Dropping the unnecessary columns.
movie_df.drop(columns=["Movie_runtime_x", "Movie_runtime_y", "Movie_genres_x"], inplace=True)





test1 = movie_df["Movie_name"] == movie_df["Primary_title"]  
test2 = movie_df["Movie_name"] == movie_df["Original_title"]

# Checking if there are any movies for which Movie_name is not either in Primary_title or Original_title
print(f"There are {(~(test1 | test2)).sum().item()} movies for which Movie_name is in neither Primary_title ot Original_title.")





movie_df.drop(columns="Movie_name", inplace=True)





movie_df.drop(columns=["Freebase_movie_ID"], inplace=True)





movie_df = pd.merge(movie_df, plot_summaries_df, on="Wikipedia_movie_ID", how="left")





movie_df.drop(columns="Wikipedia_movie_ID", inplace=True)


movie_df["Movie_release_date"] = movie_df["Movie_release_date"].astype(float)


# Check for duplicates in movie_df
movie_df.duplicated(subset=['Primary_title', 'Original_title', 'Movie_release_date']).sum()


# Get the index of the max Num_votes for duplicated rows
idx = movie_df.groupby(['Primary_title', 'Original_title', 'Movie_release_date'])['Num_votes'].idxmax()

movie_df = movie_df.loc[idx]

movie_df.reset_index(drop=True, inplace=True)

# Check again if duplicated rows are still here
movie_df.duplicated(subset=['Primary_title', 'Original_title', 'Movie_release_date']).sum()








Kaggle_df = KaggleDatasetLoader(directory).load_kaggle()
TheNumbers_df = NumbersDatasetLoader(directory).load_numbers()





Kaggle_df.head()


# Clean and convert financials to float for consistency with the movie_df
TheNumbers_df = data_utils.clean_currency_columns(TheNumbers_df, ["production_budget", "domestic_gross", "worldwide_gross"])


TheNumbers_df.head()





# Normalize the title columns for case insensitive matching
Kaggle_df = data_utils.clean_string_columns(Kaggle_df, ["original_title"])
TheNumbers_df = data_utils.clean_string_columns(TheNumbers_df, ["movie"])

# Merge both dataframes on the title columns
merged_df = pd.merge(Kaggle_df, TheNumbers_df, left_on="original_title", right_on="movie", how="outer")





merged_df["movie_budget"] = np.where((merged_df["production_budget"].notna() & (merged_df["production_budget"] != 0)),
                                     merged_df["production_budget"],
                                     merged_df["budget"])

merged_df["movie_revenue"] = np.where((merged_df["worldwide_gross"].notna() & (merged_df["worldwide_gross"] != 0)),
                                      merged_df["worldwide_gross"],
                                      merged_df["revenue"])

# Drop the original columns that were merged into new ones
merged_df.drop(columns=["budget", "revenue", "production_budget", "worldwide_gross", "movie"], inplace=True)





# Create a mask where both columns are either zero or NaN
mask = (
    (merged_df["movie_budget"].isna() | (merged_df["movie_budget"] == 0)) &
    (merged_df["movie_revenue"].isna() | (merged_df["movie_revenue"] == 0))
)

# Apply the mask and drop those rows
df_filtered = merged_df[~mask]


print("The number of available and relevant movie budget and revenue information is " + str(df_filtered["movie_budget"].size))

df_filtered.head()





# Convert the 'Release_Date' column to datetime
df_filtered['release_date'] = pd.to_datetime(df_filtered['release_date'])

# Extract the year from the datetime
df_filtered['release_date'] = df_filtered['release_date'].dt.year


# Normalize the title columns for case-insensitive matching
movie_df["Original_title"] = movie_df["Original_title"].str.strip().str.lower()
movie_df["Primary_title"] = movie_df["Primary_title"].str.strip().str.lower()

# Merging on the original title.
merge1 = pd.merge(movie_df, df_filtered, how="inner", left_on=["Original_title", "Movie_release_date"], right_on=["original_title", "release_date"])
# Merging on the secondary title.
merge2 = pd.merge(movie_df, df_filtered, how="inner", left_on=["Primary_title", "Movie_release_date"], right_on=["original_title", "release_date"])

# Concatenating and dropping duplicates that appear from movies with the same Original_title and Primary_title.
final_df = pd.concat([merge1, merge2]).drop_duplicates(subset=["tconst"]).reset_index(drop=True)

# Filter out movies where revenue data is missing or zero and prioritize initial revenue
final_df["final_movie_revenue"] = np.where(
    (final_df["Movie_box_office_revenue"].notna() & (final_df["Movie_box_office_revenue"] != 0)),
    final_df["Movie_box_office_revenue"],
    final_df["movie_revenue"]
)

final_df.drop(columns=["Movie_box_office_revenue", "domestic_gross", "movie_revenue", "original_title", "genres", "id", "release_date"], inplace=True, errors="ignore")

# Renaming the columns to follow the convention
rename_columns = {"popularity":"Popularity", 
                  "production_companies":"Production_companies", 
                  "prodcution_countries":"Production_countries",
                  "movie_budget":"Movie_budget",
                  "final_movie_revenue":"Final_movie_revenue",
                  "Movie_genres_y":"Movie_genres"}

final_df.rename(columns=rename_columns, inplace=True)


print(f"The resulting dataframe has {len(final_df):,} rows.")
final_df.head()








final_df = evaluation_utils.calculate_roi(final_df, "Movie_budget", "Final_movie_revenue")
print("There is now " + str(final_df["Final_movie_revenue"].size) + " movies to work with in the financial success analysis")





# Initialize scalers
final_df = evaluation_utils.scale_features(final_df)





# Calculate movie success which is a weighted sum choosing that the ROI feature will have double the importance of ratings
#final_df = evaluation_utils.calculate_weighted_success(final_df, 0.67)

final_df.head()




















# Initialize object for EDA processing
eda = EDA(final_df.copy())


pd.options.display.float_format = "{:,.2f}".format

# Convert columns to numeric and drop rows with missing data.
eda.dataframe[eda.numeric_columns] = eda.dataframe[eda.numeric_columns].apply(pd.to_numeric, errors="coerce")
eda.dataframe.dropna(subset=eda.numeric_columns, inplace=True)

# Summary statistics
summary_table = eda.summary()

summary_table


eda.plot_histograms(
    variables=["Average_ratings", "Movie_budget", "Final_movie_revenue", "ROI"],
    title="Figure 1: Histogram of Dependent Variables",
    bins=15,
    layout=(2, 2)
)





# Apply log transformation to skewed dependent variables
eda.dataframe = evaluation_utils.log_transform(eda.dataframe, ["ROI"])


eda.plot_histograms(
    variables=["log_ROI"],
    title="Figure 2: Histogram of Log Dependent Variable ROI",
    bins=15,
    layout=(1, 1)
)





eda.plot_histograms(
    variables=["Num_votes", "Movie_release_date", "Movie_runtime", "Female_actors", "Male_actors"],
    title="Figure 3: Histogram of Independent Variables",
    bins=15,
    layout=(2, 3)
)





# Apply log transformation to skewed independent variables
eda.dataframe = evaluation_utils.log_transform(eda.dataframe, ["Num_votes"])

eda.plot_histograms(
    variables=["log_Num_votes"],
    title="Figure 4: Histogram of Log Independent Variable Num_votes",
    bins=15,
    layout=(1, 1)
)





eda.numeric_columns = ["Average_ratings",
                   "log_Num_votes",
                   "Movie_release_date",
                   "log_ROI",
                   "Movie_runtime",
                   "Female_actors",
                   "Male_actors"]

eda.plot_boxplots()





# Filtering movies after 1970 awith runtime less than 200 minutes
eda.dataframe = eda.dataframe[(eda.dataframe["Movie_release_date"] > 1970) & (eda.dataframe["Movie_runtime"] < 200)]


print(f"After filtering the dataframe we are left with {len(eda.dataframe):,} movies.")





# eda.numeric_columns = ["Average_ratings",
#                    "log_Num_votes",
#                    "Movie_release_date",
#                    #"log_Final_movie_revenue",
#                    "log_ROI",
#                    "log_Movie_runtime",
#                    "log_Female_actors",
#                    "log_Male_actors",
#                    "Movie_success"]

# # Dataframe without outliers
# eda.dataframe = data_utils.remove_outliers(eda.dataframe, eda.numeric_columns)

# eda.plot_boxplots()


# print(f"After removing the outliers and dropping the null values, we are left with {len(eda.dataframe):,} movies.")





eda.plot_correlation_matrix()





eda.frequency_actors_gender()











filtered_genre_counts = eda.filter_and_count(column_name="Movie_genres", threshold=200)





popular_genres = eda.analyze_popular_genres(filtered_genre_counts=filtered_genre_counts)





# Creating a new column named "Main_genre" that stores, for each movie, the first occurence of one of the popular genres
eda.get_main_genres(popular_genres=popular_genres)





eda.dataframe["Movie_main_genre"] = eda.dataframe["Main_genre"] # Backup the original genre column before get_dummies
eda.dataframe = pd.get_dummies(eda.dataframe, columns=["Main_genre"], drop_first=True)
eda.dataframe.sample(5)


eda.plot_female_percentage(columns= ["Movie_release_date"], plot_type="Line")


eda.plot_female_percentage(columns=["Movie_main_genre"], plot_type = "Bar")



eda.plot_female_percentage(columns=["Movie_release_date", "Movie_main_genre"], plot_type = "Interactif by genre")








eda.analyze_languages()








eda.categorize_languages()





eda.analyze_countries()





count_usa_movies = (eda.dataframe["Movie_countries"] == "united states of america").sum()
count_other_countries_movie = len(eda.dataframe) - count_usa_movies 
print(f"There are {count_usa_movies} USA movies and {count_other_countries_movie} movies from other countries.")


eda.categorize_countries()





filtered_production_count = eda.analyze_production_companies()


prod_companies = pd.read_csv("data/Extra CSV/production_companies.csv")

box_office = eda.merge_production_data(filtered_production_count, prod_companies)






eda.unify_columbia_revenue(box_office_df=box_office)



box_office["Total Worldwide Box Office"] = box_office["Total Worldwide Box Office"].replace(
    {"\$": "", ",": ""}, regex=True).astype(float)

def calculate_box_office(companies_list):
   if not companies_list:
        return 0  
   return box_office.loc[box_office["Company Name"].isin(companies_list), "Total Worldwide Box Office"].sum()

eda.dataframe["Box_office_companies"] = eda.dataframe["Production_companies_cleaned"].apply(calculate_box_office)
eda.filter_movies_with_box_office(box_office_column="Box_office_companies")


eda.plot_gender_comparison(columns=["log_ROI", "Normalized_Rating"])


eda.plot_gender_comparison(columns=["log_ROI", "Normalized_Rating"], interactive=True)





print(eda.dataframe.columns)









final_df_regression = eda.dataframe.copy()

# Calculating the Movie Success metric
final_df_regression = evaluation_utils.calculate_weighted_success(final_df_regression, 0.67)








# Define independent and dependent variables
indep_vars = ["Female_actors", "Male_actors", "Movie_runtime", "Is_not_only_english", 
                   "Is_USA_movie",  "Main_genre_drama",
                   "Main_genre_thriller", "Main_genre_comedy", "Main_genre_fantasy", "Main_genre_horror", "Movie_release_date",
                   "Box_office_companies", "log_Num_votes"]

dep_var = "log_ROI"

# Run the regression along with necessary preprocessing and metrics
regression.run_regression(final_df_regression, indep_vars, dep_var)








# Removing log_Num_votes as it is involved in the calculation of Normalized_Rating and Movie_success
indep_vars.remove("log_Num_votes")

# Define new dependent variables
dep_vars = ["Normalized_Rating", "Movie_success"]

show_VIF = True
for dep_var in dep_vars:
    # Run the regression along with necessary preprocessing and metrics
    regression.run_regression(final_df_regression, indep_vars, dep_var, show_VIF)
    
    # Only display VIF information once as the independent variables have not changed
    show_VIF = False

















# Setting up the genre columns
genre_columns = ["Main_genre_drama", "Main_genre_thriller", "Main_genre_comedy", 
                "Main_genre_fantasy", "Main_genre_horror"]

# Define variables for regression on individual genres
indep_vars = ["Female_actors", "Male_actors", "Movie_runtime", "Is_not_only_english", 
                   "Is_USA_movie", "Movie_release_date","Box_office_companies", "log_Num_votes"]

# Setting up the dependent variable
dep_var = "log_ROI"

show_VIF = True
for genre in genre_columns:
    # Run the genre specific regression along with necessary preprocessing and metrics
    regression.run_regression(final_df_regression, indep_vars, dep_var, show_VIF, genre)
    
    # Only display VIF information once as the independent variables have not changed
    show_VIF = False





# Removing log_Num_votes as it is involved in the calculation of Movie_success
indep_vars.remove("log_Num_votes")

# Setting up the dependent variable
dep_var = "Movie_success"

show_VIF = True
for genre in genre_columns:
    # Run the genre specific regression along with necessary preprocessing and metrics
    regression.run_regression(final_df_regression, indep_vars, dep_var, show_VIF, genre)
    
    # Only display VIF information once as the independent variables have not changed
    show_VIF = False











streaming_df = get_streaming_dataframe()


streaming_df.head()


streaming_df.drop_duplicates(inplace=True)






eda_streaming = EDA(streaming_df.copy())



pd.options.display.float_format = "{:,.2f}".format

# Filtrer uniquement les colonnes qui existent dans la dataframe
existing_numeric_columns = [col for col in eda_streaming.numeric_columns if col in eda_streaming.dataframe.columns]

# Convert columns to numeric for existing columns and drop rows with missing data
eda_streaming.dataframe[existing_numeric_columns] = eda_streaming.dataframe[existing_numeric_columns].apply(pd.to_numeric, errors="coerce")
eda_streaming.dataframe.dropna(subset=existing_numeric_columns, inplace=True)

# Summary statistics
summary_table = eda_streaming.summary_bis()

summary_table



variables = ["Num_votes", "Movie_release_date", "Movie_runtime", "Female_actors", "Male_actors"]

eda_streaming.plotly_kde(
    variables=variables,
    bins=15,
    second_dataframe=eda.dataframe,
    save_html=False,
    output_dir="./histograms_plots"
)






# Apply log transformation to skewed independent variables
eda_streaming.dataframe = evaluation_utils.log_transform(eda_streaming.dataframe, ["Num_votes", "Movie_runtime", "Female_actors", "Male_actors"])







##eda_streaming.plot_log_transformed_independent_histograms()





eda_streaming.numeric_columns = ["Average_ratings",
                   "log_Num_votes",
                   "Movie_release_date",
                   #"log_Final_movie_revenue",
                   #"log_ROI",   # je crois juste lui je le teje 
                   "log_Movie_runtime",
                   "log_Female_actors",
                   "log_Male_actors",
                   #"Movie_success"
                   ]

eda_streaming.plot_boxplots()






eda_streaming.dataframe = data_utils.remove_outliers(eda_streaming.dataframe, eda_streaming.numeric_columns)

eda_streaming.plot_boxplots()


print(f"After removing the outliers and dropping the null values, we are left with {len(eda_streaming.dataframe):,} movies.")





eda_streaming.plot_correlation_matrix()














eda_streaming.analyze_female_actors_by_genre()


eda_streaming.plot_female_percentage_evolution()


filtered_genre_counts = eda_streaming.filter_and_count(column_name="Movie_genres", threshold=500)





popular_genres = eda_streaming.analyze_popular_genres(filtered_genre_counts=filtered_genre_counts)








eda_streaming.dataframe = eda_streaming.dataframe[eda_streaming.dataframe["Movie_genres"].apply(lambda x: len(x.split(",")) <= 2)]


eda_streaming.dataframe["Movie_genres"].value_counts()



dummies = pd.get_dummies(eda_streaming.dataframe["Movie_genres"], prefix="Movie_genres", drop_first=True)

dummy_columns = dummies.columns.tolist()
print("Dummies columns :", dummy_columns)



#eda_streaming.dataframe = pd.get_dummies(eda_streaming.dataframe, columns=["Movie_genres"], drop_first=True) 
eda_streaming.dataframe = pd.concat([eda_streaming.dataframe, dummies], axis=1)

eda_streaming.dataframe.sample(5)








eda_streaming.analyze_languages()





eda_streaming.categorize_languages()





eda_streaming.plot_average_rating_by_country(country_column="Movie_countries", rating_column="Average_ratings", color_scale="Purples")


eda_streaming.analyze_countries_plotly()





count_usa_movies = (eda_streaming.dataframe["Movie_countries"] == "United States").sum()
count_other_countries_movie = len(eda_streaming.dataframe) - count_usa_movies 
print(f"There are {count_usa_movies} USA movies and {count_other_countries_movie} movies from other countries.")


eda_streaming.categorize_countries_bis()








eda_streaming.dataframe = pd.get_dummies(eda_streaming.dataframe, columns=["Platform"], drop_first=True) 
eda_streaming.dataframe.sample(5)





streaming_df_regression = eda_streaming.dataframe.copy()


print(streaming_df_regression.columns)


indep_vars1=["log_Female_actors", "log_Male_actors", "Num_votes", "log_Movie_runtime", "Is_not_only_english", "Is_USA_movie",
          'Platform_Disney', 'Platform_Netflix', "Is_Adult"]

indep_vars=np.hstack([indep_vars1, dummy_columns])
dep_vars = "Average_ratings"

X = streaming_df_regression[indep_vars]
y = streaming_df_regression[dep_vars]

scaler = StandardScaler()
X_scaled_streaming = scaler.fit_transform(X)

X_scaled_streaming = pd.DataFrame(X_scaled_streaming, columns=X.columns, index=X.index)

X_scaled_wconst_streaming = sm.add_constant(X_scaled_streaming)


# Create a DataFrame to store VIF values
vif_data = pd.DataFrame()
vif_data["Feature"] = X_scaled_streaming.columns

# Calculate VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(X_scaled_streaming.values, i) for i in range(X_scaled_streaming.shape[1])]

# Display VIF values
print(vif_data)


model_streaming = sm.OLS(y, X_scaled_wconst_streaming).fit()
print(model_streaming.summary())














bechdel_loader = BechdelDatasetLoader(directory)
bechdel_df = bechdel_loader.load_bechdel()
bechdel_df.head()


eda_sexualization = EDA(eda.dataframe.copy())
eda_sexualization.dataframe.head()


eda_sexualization_regression = evaluation_utils.calculate_weighted_success(eda_sexualization.dataframe, 0.67)


bechdel_final_merge = pd.merge(eda_sexualization_regression, bechdel_df, on="tconst")
bechdel_final_merge.head()


bechdel_final_merge = bechdel_final_merge.drop(columns=["tconst"])





indep_vars = ["Female_actors", "Male_actors", "Movie_runtime", "Is_not_only_english", 
                   "Is_USA_movie", "Main_genre_drama",
                   "Main_genre_thriller", "Main_genre_comedy", "Main_genre_fantasy", "Main_genre_horror", "Start_year",
                   "Box_office_companies", "Bechdel_rating", "log_Num_votes"]

dep_var = "log_ROI"
regression.run_regression(bechdel_final_merge, indep_vars, dep_var)





indep_vars = ["Female_actors", "Male_actors", "Movie_runtime", "Is_not_only_english", 
                   "Is_USA_movie", "Main_genre_drama",
                   "Main_genre_thriller", "Main_genre_comedy", "Main_genre_fantasy", "Main_genre_horror", "Start_year",
                   "Box_office_companies", "Bechdel_rating", "log_Num_votes"]

dep_var = "Normalized_Rating"
regression.run_regression(bechdel_final_merge, indep_vars, dep_var)





indep_vars = ["Female_actors", "Male_actors", "Movie_runtime", "Is_not_only_english", 
                   "Is_USA_movie", "Main_genre_drama",
                   "Main_genre_thriller", "Main_genre_comedy", "Main_genre_fantasy", "Main_genre_horror", "Start_year",
                   "Box_office_companies", "Bechdel_rating", "log_Num_votes"]

dep_var = "Movie_success"
regression.run_regression(bechdel_final_merge, indep_vars, dep_var)











dialogue_loader = DialogueDatasetLoader(directory)
movie_dialogue_df = dialogue_loader.load_movie_dialogue()
print(f"There are {len(movie_dialogue_df)} movies.")


character_dialogue_df = dialogue_loader.load_character_dialogue()
print(f"There are {len(character_dialogue_df)} characters.")





dialogue_df = pd.merge(movie_dialogue_df, character_dialogue_df, on="Script_id")
dialogue_df.drop(columns=["Script_id"], inplace = True)

dialogue_gender_words_df = dialogue_df.drop(columns=["Character_name", "Age", "Year", "Title"])
dialogue_gender_words_df.head()


print(f"The resulting dataframe has {len(dialogue_gender_words_df)} rows.")





dialogue_merged_df = pd.merge(eda_sexualization.dataframe, dialogue_gender_words_df, on="tconst")
print(f"The resulting dataframe has {len(dialogue_merged_df)} movies.")


dialogue_merged_df.head()


eda_dialogue = EDA(dialogue_merged_df)





eda_dialogue_words = EDA(eda_dialogue.count_gender_words())





eda_dialogue_words.plotly_gender_words()


#ANALYSIS OF GRAPH








character_types = CMU_loader.load_character_types()
character_types.head()


print(f"There are {len(character_types)} samples in character_types")


empty_actor_names = (character_types['Actor_name'].isna() | (character_types['Actor_name'] == '')).sum()
empty_map_ids = (character_types['Freebase_character_actor_map_ID'].isna() | (character_types['Freebase_character_actor_map_ID'] == '')).sum()
empty_character_names = (character_types['Character_name'].isna() | (character_types['Character_name'] == '')).sum()

print(f"There are {empty_actor_names} NaN actor names.")
print(f"There are {empty_map_ids} NaN freebase_character_actor_map_id.")
print(f"There are {empty_character_names} NaN character names.")





character_metadata_sexualization_study = CMU_loader.load_character_metadata()
character_metadata_sexualization_study.head()


merged_character_df = pd.merge(character_types, character_metadata_sexualization_study, on=["Freebase_character_actor_map_ID"], how="inner")
print(f"There are {len(merged_character_df)} matches between the two dataframes.")


gender_counts = merged_character_df.groupby(['Actor_gender']).size()
print(gender_counts)





sexualization_study_sample = final_df.head(50)
sexualization_study_sample.head()


labels1 = ['sexualization', 'no sexualization']
zeroshot_loader = ZeroShotResultsLoader(directory)
zeroshot_df1 = zeroshot_loader.load_results(1)
zeroshot_df1[['Plot Summaries','sexualization_prediction','sexualization_confidence']].head(10)


labels2 = ['gender stereotype', 'no gender stereotype']
zeroshot_df2 = zeroshot_loader.load_results(2)
zeroshot_df2[['Plot Summaries','sexualization_prediction','sexualization_confidence']].head(10)


below_80 = zeroshot_df1[zeroshot_df1['sexualization_confidence'] < 0.80]
print(f"Percentage of samples for the first set of labels with sexualization_confidence below 80% is {below_80.shape[0]*100/len(zeroshot_df1):.2f}%")


below_80 = zeroshot_df2[zeroshot_df2['sexualization_confidence'] < 0.80]
print(f"Percentage of samples for the second set of labels with sexualization_confidence below 80% is {below_80.shape[0]*100/len(zeroshot_df2):.2f}%")





print(zeroshot_df1["Plot Summaries"][1])








filtered_genre_counts = eda_sexualization.filter_and_count(column_name="Movie_genres", threshold=500)
popular_genres = eda_sexualization.analyze_popular_genres(column_name="Movie_genres", filtered_genre_counts=filtered_genre_counts)
main_genres = eda_sexualization.get_main_genres(column_name="Movie_genres", popular_genres = popular_genres)








'''
for genre, clusters in genre_cluster_details.items():
    print(f"Top terms for {genre}:")
    for cluster, terms in clusters.items():
        print(f"  Cluster {cluster}: {', '.join(terms)
'''}")






